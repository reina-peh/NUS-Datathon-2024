{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### NUS_DATATHON_SINGLIFE\n",
        "Team Name : **TeamZero**\n",
        "\n",
        "This notebook provides a detailed overview of our approach in the NUS DATATHON SINGLIFE Competition, focusing on key aspects of data handling and model development:\n",
        "\n",
        "**Data Analysis:** We delve into the dataset to explore and understand its key features, gaining insights into the underlying patterns and distributions.\n",
        "\n",
        "**Handling Imbalanced Data:** Strategies are implemented to effectively manage data imbalance, ensuring our model's robustness and accuracy.\n",
        "\n",
        "**Feature Selection:** Through careful analysis, we identify and select the most impactful features that contribute significantly to our model's performance.\n",
        "\n",
        "**Model Training & Tuning:** We develop a predictive model, followed by meticulous tuning to enhance its predictive capabilities and ensure optimal performance.\n",
        "\n",
        "### Methodological Justifications and Detailed Explanations\n",
        "In this project, we have employed a variety of data preprocessing and modeling techniques. Each method was selected based on specific characteristics of our dataset and the objectives of our analysis. To ensure clarity and transparency, we have provided detailed justifications for our methodological choices. These include, but are not limited to:\n",
        "1. Imputation Techniques\n",
        "2. Feature Selection\n",
        "3. Model Selection\n",
        "4. Parameter Tuning\n",
        "\n",
        "For full details, please visit our GitHub repository: https://github.com/reina-peh/NUS-Datathon-2024\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "### **Important Reminder:**\n",
        "Before proceeding to the ```train_model``` and ```test_hidden_data``` sections, please ensure that all the preceding functions in this notebook have been executed. Running these functions is crucial for setting up the necessary environment and preprocessing steps required for the model training and testing phases.\n",
        "\n",
        "<br>\n",
        "\n",
        "### LET'S BEGIN!\n"
      ],
      "metadata": {
        "id": "qOSJqvvFEvtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "-42Vu33vBmeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UezsrxLvHj3w"
      },
      "source": [
        "### **Data Cleaning**\n",
        "\n",
        "### Function 1: ```clean_data(data, target)```\n",
        "\n",
        "This function targets null values and specific columns for removal. The steps include:\n",
        "\n",
        "1. Null Value Analysis: It calculates and displays the count and percentage of null values per column.\n",
        "2. Column Removal: Columns with 100% null values are removed, except for a specified target column. Additional columns deemed redundant ('hh_20', 'pop_20', 'hh_size_est') are also dropped.\n",
        "\n",
        "### *Function* 2: ```clean_data_v2(data)```\n",
        "\n",
        "The focus here is on handling 'None' entries, data type conversion, and removing a unique identifier column.\n",
        "\n",
        "1. 'None' Entries Handling: Counts and percentages of 'None' entries per column are calculated and sorted.\n",
        "Row Removal: Rows where 'min_occ_date' or 'cltdob_fix' are 'None' are removed, indicating the importance of these fields.\n",
        "2. Data Type Optimization: Converts all float64 columns to float32 for efficiency.\n",
        "3. Column Dropping: The 'clntnum' column, a unique identifier, is dropped as it does not contribute to the analysis.\n",
        "\n",
        "### Function 3: ```clean_target_column(data, target_column_name) ```\n",
        "This function is dedicated to preprocessing the target column of the dataset. The primary focus is to handle missing values and ensure the data type consistency of the target variable, which is crucial for the accuracy and effectiveness of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuvrwmgGqyOJ"
      },
      "outputs": [],
      "source": [
        "def check_duplicates(data, column_name):\n",
        "    \"\"\"\n",
        "    Check for duplicates in a specified column of a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    data (DataFrame): The DataFrame to check for duplicates.\n",
        "    column_name (str): The name of the column to check for duplicates.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the number of duplicates and a boolean indicating if any duplicates are present.\n",
        "    \"\"\"\n",
        "    # Check for duplicate values in the specified column\n",
        "    duplicates = data[column_name].duplicated()\n",
        "\n",
        "    # Count the number of duplicate values\n",
        "    num_duplicates = duplicates.sum()\n",
        "\n",
        "    # Check if there are any duplicates\n",
        "    any_duplicates = duplicates.any()\n",
        "\n",
        "    return num_duplicates, any_duplicates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n29QiJZaqyOK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def clean_data(data, target):\n",
        "    \"\"\"\n",
        "    Cleans the provided DataFrame by dropping columns with certain criteria.\n",
        "\n",
        "    Parameters:\n",
        "    data (DataFrame): The DataFrame to clean.\n",
        "    target (str): The name of the target column to preserve.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: The cleaned DataFrame.\n",
        "    \"\"\"\n",
        "    # Count the number of null values per column\n",
        "    null_counts = data.isnull().sum()\n",
        "\n",
        "    # Calculate the percentage of null values\n",
        "    total_rows = len(data)\n",
        "    null_percentage = (null_counts / total_rows) * 100\n",
        "\n",
        "    # Combine the counts and percentages into a DataFrame for a better display\n",
        "    null_stats = pd.DataFrame({'Number of Nulls': null_counts, 'Percentage of Nulls': null_percentage})\n",
        "\n",
        "    # Sort by 'Percentage of Nulls'\n",
        "    null_stats.sort_values(by='Percentage of Nulls', ascending=False, inplace=True)\n",
        "\n",
        "    # Find columns where the percentage of null values is 100%\n",
        "    columns_to_drop = null_percentage[null_percentage == 100].index\n",
        "\n",
        "    # Ensure the target column is not in the list of columns to drop\n",
        "    columns_to_drop = [col for col in columns_to_drop if col != target]\n",
        "\n",
        "    # Drop these columns\n",
        "    data_cleaned = data.drop(columns=columns_to_drop)\n",
        "\n",
        "    # Drop 'hh_20', 'pop_20', 'hh_size_est' since hh_20 divided by pop_20 gives\n",
        "    # values in an existing columns 'hh_size' and 'hh_size_est' contains values rounded off from 'hh_size'\n",
        "    columns_to_drop_also = ['hh_20', 'pop_20', 'hh_size_est']\n",
        "    data_cleaned.drop(columns=columns_to_drop_also, axis=1, inplace=True)\n",
        "\n",
        "    return data_cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkyS-Ww1qyOK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def clean_data_v2(data):\n",
        "    \"\"\"\n",
        "    Further cleans the provided DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    data (DataFrame): The DataFrame to clean.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: The cleaned DataFrame.\n",
        "    \"\"\"\n",
        "    # Count the number of 'None' entries per column\n",
        "    none_counts = data.apply(lambda x: (x == 'None').sum())\n",
        "\n",
        "    # Calculate the percentage of 'None' entries\n",
        "    total_rows = len(data)\n",
        "    none_percentage = (none_counts / total_rows) * 100\n",
        "\n",
        "    # Combine the counts and percentages into a DataFrame for a better display\n",
        "    none_stats = pd.DataFrame({'Number of None Entries': none_counts, 'Percentage of None Entries': none_percentage})\n",
        "\n",
        "    # Sort by 'Percentage of None Entries'\n",
        "    none_stats.sort_values(by='Percentage of None Entries', ascending=False, inplace=True)\n",
        "\n",
        "    # Check if either 'min_occ_date' or 'cltdob_fix' have 'None' values\n",
        "    condition = (data['min_occ_date'] == 'None') | (data['cltdob_fix'] == 'None')\n",
        "\n",
        "    # Remove rows that satisfy the condition\n",
        "    data = data[~condition]\n",
        "\n",
        "    # Convert all float64 columns to float32\n",
        "    float_cols = data.select_dtypes(include=['float64']).columns\n",
        "    data[float_cols] = data[float_cols].astype('float32')\n",
        "\n",
        "    # Drop 'clntnum' column\n",
        "    data.drop('clntnum', axis=1, inplace=True)\n",
        "\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXn5A8eZCsTo"
      },
      "outputs": [],
      "source": [
        "def clean_target_column(data, target_column_name):\n",
        "\n",
        "    # Replace NaN values with 0\n",
        "    data[target_column_name] = data[target_column_name].fillna(0)\n",
        "\n",
        "    # Convert 1.0 to 1\n",
        "    data[target_column_name] = data[target_column_name].apply(lambda x: 1 if x == 1.0 else x)\n",
        "\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moVOincQHs5p"
      },
      "source": [
        "###Feature Engineering\n",
        "\n",
        "We have identified that age at which a customer purchases an insurance policy is important, hence we created a new column to calculate their age using ```min_occ_date``` and ```cltdob_fix```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3W8G9WuGqyOL"
      },
      "outputs": [],
      "source": [
        "# feature engineering\n",
        "\n",
        "def calculate_age(data, date_column_1, date_column_2):\n",
        "\n",
        "    # Ensure the date columns are in datetime format\n",
        "    data[date_column_1] = pd.to_datetime(data[date_column_1])\n",
        "    data[date_column_2] = pd.to_datetime(data[date_column_2])\n",
        "\n",
        "    # Calculate age in decimal years\n",
        "    data['client_age'] = (data[date_column_1] - data[date_column_2]).astype('timedelta64[Y]').astype('int')\n",
        "\n",
        "    # Drop the specified date columns\n",
        "    data.drop(columns=[date_column_1, date_column_2], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train Test Split to divide a dataset into training and testing subsets for model training and evaluation**"
      ],
      "metadata": {
        "id": "ENPifbbJ_dkZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndmhOnGUqyOL"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def create_data_split(data, target, test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Splits the data into training and validation sets and defines the features.\n",
        "\n",
        "    Parameters:\n",
        "    data (DataFrame): The cleaned DataFrame to be split.\n",
        "    target (str): The name of the target column.\n",
        "    test_size (float, optional): The proportion of the dataset to include in the validation split. Default is 0.2.\n",
        "    random_state (int, optional): The random state for reproducibility. Default is 42.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the training set, validation set, and features list.\n",
        "    \"\"\"\n",
        "    # Create single fold split\n",
        "    tr, val = train_test_split(data, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    # Define the features\n",
        "    features = [col for col in data.columns if col != target]\n",
        "\n",
        "    return tr, val, features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Imputation: Addressing NaN or None Values**\n",
        "\n",
        "**Challenges with Missing Data:** Our dataset contained NaN or None values, which posed a significant challenge for performing accurate statistical analyses and data visualizations. The choice of imputation technique was critical to preserve data integrity and maintain the reliability of our analyses.\n",
        "\n",
        "**Why Median Imputation?**\n",
        "\n",
        "- **Robustness Against Outliers and Skewed Distributions:** Median imputation was chosen for its robustness in the presence of outliers and skewed data distributions. Unlike the mean, which can be heavily influenced by extreme values, the median provides a more representative value of the central tendency in such cases.\n",
        "\n",
        "- **Maintaining Data Integrity:** The median imputation helps in preserving the original distribution of the dataset. This is crucial for maintaining the structural integrity of the data, ensuring that subsequent analyses are reflective of the true nature of the underlying data.\n",
        "\n",
        "**Alternatives Considered and Their Limitations:**\n",
        "\n",
        "- **Mean Imputation:** Simple imputer mean was considered; however, its susceptibility to outliers made it less suitable for our dataset. Mean imputation could potentially introduce bias, especially in skewed distributions, leading to distorted analyses.\n",
        "\n",
        "- **Simplicity and Efficiency:** Given the size and nature of our dataset, median imputation offered a balance between simplicity, computational efficiency, and effectiveness. This method allowed us to quickly and effectively address missing values, enabling us to proceed with our analyses without introducing significant bias.\n"
      ],
      "metadata": {
        "id": "IZT2jGOEKGD_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qpM8CrpqyOM"
      },
      "outputs": [],
      "source": [
        "from sklearn.experimental import enable_iterative_imputer  # Import to enable IterativeImputer\n",
        "from sklearn.impute import SimpleImputer, IterativeImputer\n",
        "import numpy as np\n",
        "\n",
        "def impute_data(tr, features, val=None):\n",
        "    \"\"\"\n",
        "    Imputes missing values in numeric and non-numeric features in the training dataset and, optionally, the validation dataset.\n",
        "\n",
        "    Parameters:\n",
        "    tr (DataFrame): The training dataset.\n",
        "    features (list of str): The list of feature names.\n",
        "    val (DataFrame, optional): The validation dataset. If None, only the training dataset will be imputed.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame, (DataFrame): The imputed training dataset and, if provided, the imputed validation dataset.\n",
        "    \"\"\"\n",
        "    # Identify numeric and non-numeric features\n",
        "    numeric_features = tr[features].select_dtypes(include=[np.number]).columns\n",
        "    non_numeric_features = tr[features].select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "    # Impute numeric features using Iterative Imputer\n",
        "    imputer_numeric = IterativeImputer(max_iter=10, initial_strategy='median', random_state=0)\n",
        "    tr[numeric_features] = imputer_numeric.fit_transform(tr[numeric_features])\n",
        "\n",
        "    # Impute non-numeric features using Simple Imputer\n",
        "    imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
        "    tr[non_numeric_features] = imputer_categorical.fit_transform(tr[non_numeric_features])\n",
        "\n",
        "    if val is not None:\n",
        "        val[numeric_features] = imputer_numeric.transform(val[numeric_features])\n",
        "        val[non_numeric_features] = imputer_categorical.transform(val[non_numeric_features])\n",
        "        return tr, val\n",
        "    else:\n",
        "        return tr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Under-Over Sampling Technique\n",
        "\n",
        "In our data preprocessing phase, we implemented a combined under-over sampling strategy to address the class imbalance in our dataset. Class imbalance is a common issue in machine learning, where some classes are underrepresented compared to others. This imbalance can lead to biased models that don't perform well on minority classes.\n",
        "\n",
        "**Why We Chose Under-Over Sampling:**\n",
        "\n",
        "- **Under-Sampling**: We first applied Random Under-Sampling to reduce the size of the overrepresented class. This approach helps in balancing the class distribution and reducing the training dataset size, which can be beneficial for computational efficiency.\n",
        "\n",
        "- **Over-Sampling with SMOTENC**: After under-sampling, we used SMOTENC (Synthetic Minority Over-sampling Technique for Nominal and Continuous data) for over-sampling the minority class. Unlike basic over-sampling techniques, SMOTENC generates synthetic samples for the minority class in a more sophisticated manner, considering both nominal and continuous features. This leads to a more balanced and representative training dataset.\n",
        "\n",
        "- **Combining Both Techniques**: By combining under-sampling and over-sampling, we aimed to create a more balanced dataset without losing significant information. This combination helps in improving the model's performance, especially its ability to predict minority class instances, leading to more reliable and generalized outcomes.\n",
        "\n",
        "By carefully addressing the class imbalance using this combined approach, we enhanced the model's ability to learn from a more representative dataset, thereby improving its predictive performance on unseen data.\n"
      ],
      "metadata": {
        "id": "_x81S0m4YZkq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0_Fp2ZV5KJy"
      },
      "outputs": [],
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import SMOTENC\n",
        "\n",
        "def under_over_sampling(tr, features, target, under_sampling_strategy=0.1, random_state=42):\n",
        "    \"\"\"\n",
        "    Applies undersampling and then oversampling to the training dataset.\n",
        "\n",
        "    Parameters:\n",
        "    tr (DataFrame): The training dataset.\n",
        "    features (list of str): The list of feature names.\n",
        "    target (str): The name of the target column.\n",
        "    under_sampling_strategy (float, optional): The sampling strategy for undersampling. Default is 0.1.\n",
        "    random_state (int, optional): The random state for reproducibility. Default is 42.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame, array: The resampled features DataFrame and target array.\n",
        "    \"\"\"\n",
        "    # Initialize RandomUnderSampler\n",
        "    rus = RandomUnderSampler(sampling_strategy=under_sampling_strategy, random_state=random_state)\n",
        "\n",
        "    # Extract features and target\n",
        "    X = tr[features]\n",
        "    y = tr[target].to_numpy()\n",
        "\n",
        "    # Apply undersampling\n",
        "    X_under, y_under = rus.fit_resample(X, y)\n",
        "\n",
        "    # Convert object types to string\n",
        "    for col in X_under.select_dtypes(include='object'):\n",
        "        X_under[col] = X_under[col].astype(str)\n",
        "\n",
        "    # Identify categorical feature indices for SMOTENC\n",
        "    categorical_features_indices = [X_under.columns.get_loc(col) for col in X_under.columns if X_under[col].dtype == 'object']\n",
        "\n",
        "    # Initialize SMOTENC with the correct indices\n",
        "    smote_nc = SMOTENC(categorical_features=categorical_features_indices, random_state=random_state)\n",
        "\n",
        "    # Apply SMOTENC for oversampling\n",
        "    X_resampled, y_resampled = smote_nc.fit_resample(X_under, y_under)\n",
        "\n",
        "    return X_resampled, y_resampled\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z05VE6boXzjy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def extract_numeric_features(data, features):\n",
        "    \"\"\"\n",
        "    Extracts numeric features from a specified subset of a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    data (DataFrame): The DataFrame from which to extract numeric features.\n",
        "    features (list of str): The list of feature names to consider for extraction.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: A DataFrame containing only the numeric features from the specified subset.\n",
        "    \"\"\"\n",
        "    # Filter the DataFrame to include only the specified features\n",
        "    data_filtered = data[features]\n",
        "\n",
        "    # Identify numeric features from the filtered DataFrame\n",
        "    numeric_features = data_filtered.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "    # Create a DataFrame with only numeric features from the filtered subset\n",
        "    numeric_data = data_filtered[numeric_features]\n",
        "\n",
        "    return numeric_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Feature Selection**\n",
        "\n",
        "**Understanding Key Influencers in High-Dimensional Data:** In our project, the primary challenge was to decipher the most influential factors from a dataset that originally contained over 200 columns. Such a high-dimensional dataset can obscure crucial insights, especially when analyzing complex customer behaviors. To navigate this, a strategic approach to feature selection was essential.\n",
        "\n",
        "**Integrating XGBClassifier with SelectFromModel:**\n",
        "\n",
        "- **Utilizing a Strong Classifier:** We employed the XGBClassifier, renowned for its effectiveness in classification tasks and its capability to rank feature importance. XGBoost, with its gradient boosting framework, excels in handling various types of data and uncovering complex patterns. Its intrinsic feature importance metric provides a reliable basis for feature selection.\n",
        "\n",
        "- **SelectFromModel Methodology:** The SelectFromModel method was applied in tandem with the XGBClassifier. This method analyzes the feature importance scores generated by the classifier and retains only the most significant features. For our project, we chose to keep the top 40 features. This threshold was thoughtfully selected to ensure that we retain enough features to capture the diverse aspects of customer behavior while avoiding the pitfalls of model overcomplexity and potential overfitting.\n",
        "\n",
        "**Why SelectFromModel Over RFE or PCA?**\n",
        "\n",
        "- **Computational Efficiency:** Recursive Feature Elimination (RFE) is inherently iterative and computationally demanding, especially with a large number of features. In contrast, SelectFromModel offers a more computationally efficient alternative.\n",
        "\n",
        "- **Preserving Interpretability with PCA Limitations:** While PCA is effective for reducing dimensionality, it transforms the original features into principal components, which can be challenging to interpret, especially in a business context where understanding specific feature influences is crucial. SelectFromModel maintains the original features, making the results more interpretable and actionable.\n",
        "\n",
        "- **Balancing Feature Set and Model Complexity:** The goal was to distill the dataset to a manageable number of features without losing critical information. SelectFromModel, coupled with XGBClassifier, provided a more nuanced approach to achieving this balance compared to the broad dimensionality reduction offered by PCA or the intensive feature elimination process of RFE.\n",
        "\n",
        "**Outcome and Impact:**\n",
        "\n",
        "By implementing this feature selection strategy, we were able to significantly reduce the feature space from over 200 to 40, focusing on the most relevant variables that influence customer behavior. This not only enhanced the model's performance by reducing noise and complexity but also aided in interpretability, allowing for more straightforward insights and decision-making based on the model's outputs.\n"
      ],
      "metadata": {
        "id": "bZHSka1___fC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qdbyqpoca6aK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "def select_features(X_train_numeric, y_resampled, X_val_numeric=None, max_features=40):\n",
        "    \"\"\"\n",
        "    Applies feature selection using SelectFromModel with XGBClassifier on the numeric features.\n",
        "    Optionally applies the same selection to a provided validation dataset.\n",
        "\n",
        "    Parameters:\n",
        "    X_train_numeric (DataFrame): Numeric features of the resampled training dataset.\n",
        "    y_resampled (array): The target array after resampling.\n",
        "    X_val_numeric (DataFrame, optional): Numeric features of the validation dataset.\n",
        "    max_features (int, optional): The maximum number of features to select.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame, SelectFromModel, (DataFrame): Transformed training dataset, the feature selector, and, if provided, transformed validation dataset.\n",
        "    \"\"\"\n",
        "    # Initialize SelectFromModel with XGBClassifier\n",
        "    feature_selector = SelectFromModel(\n",
        "        estimator=XGBClassifier(),\n",
        "        threshold=-np.inf,\n",
        "        max_features=max_features\n",
        "    )\n",
        "\n",
        "    # Apply feature selection to the training data\n",
        "    X_train_selected = feature_selector.fit_transform(X_train_numeric, y_resampled)\n",
        "\n",
        "    # If validation data is provided, apply the same feature selection to it\n",
        "    X_val_selected = None\n",
        "    if X_val_numeric is not None:\n",
        "        X_val_selected = feature_selector.transform(X_val_numeric)\n",
        "\n",
        "    return X_train_selected, feature_selector, X_val_selected\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Training**\n",
        "\n",
        "In the training stage, we opted for the XGBoost classifier, guided by a comparative performance analysis against models like Logistic Regression, SVM, and KNN. The `pretrain_model` function is responsible for initializing the XGBClassifier with chosen hyperparameters and fitting it to the training data. Below are the key theoretical reasons for selecting XGBoost:\n",
        "\n",
        "1. **Ensemble Learning Method**: XGBoost leverages an advanced form of gradient boosting, an ensemble technique where multiple weak learners (decision trees) are combined in a sequential manner. Each tree in the sequence corrects the errors of its predecessors, culminating in a robust overall model.\n",
        "\n",
        "2. **Regularization Techniques**: It includes L1 and L2 regularization, which significantly reduce the risk of overfitting, a crucial advantage over traditional models, particularly in complex datasets.\n",
        "\n",
        "3. **Versatile Data Handling**: XGBoost efficiently processes a mix of categorical and numerical features, outperforming models like Logistic Regression and KNN that often require extensive preprocessing for different data types.\n",
        "\n",
        "4. **Optimized Performance and Speed**: Designed for high performance and speed, XGBoost optimizes computational resources and memory usage, making it more scalable and faster than conventional algorithms, especially in handling large datasets.\n",
        "\n",
        "These factors combined — the ensemble approach, regularization, versatility in data handling, and optimized performance — make XGBoost an excellent choice for our dataset, leading to superior accuracy and efficiency compared to the alternatives.\n"
      ],
      "metadata": {
        "id": "HcdHBum7LfRY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EzXbMsnqyOM",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "def pretrain_model(X_train_selected, y_resampled):\n",
        "    \"\"\"\n",
        "    Trains an XGBClassifier on the selected training features.\n",
        "\n",
        "    Parameters:\n",
        "    X_train_selected (DataFrame or array): Selected features of the training dataset.\n",
        "    y_resampled (array): The target array after resampling.\n",
        "\n",
        "    Returns:\n",
        "    XGBClassifier: The trained XGBClassifier model.\n",
        "    \"\"\"\n",
        "\n",
        "    clf = xgb.XGBClassifier(\n",
        "        objective = 'binary:logistic',\n",
        "        learning_rate = 0.22145013353887077,\n",
        "        gamma = 4.3039462306959395,\n",
        "        max_depth = 8,\n",
        "        min_child_weight = 4,\n",
        "        subsample = 0.6099674904240726,\n",
        "        colsample_bytree = 0.6033130913572766,\n",
        "        reg_lambda = 0.0005851643553019946,\n",
        "        reg_alpha = 0.04115841889918572,\n",
        "        use_label_encoder = False,\n",
        "        eval_metric = 'logloss'\n",
        "    )\n",
        "\n",
        "    clf.fit(X_train_selected, y_resampled)\n",
        "\n",
        "    return clf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBW7tVXvqyOM"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def predict_evaluate(model, X_val_selected, val_target):\n",
        "    \"\"\"\n",
        "    Makes predictions with the trained model on the validation set and evaluates them.\n",
        "\n",
        "    Parameters:\n",
        "    model (XGBClassifier): The trained XGBClassifier model.\n",
        "    X_val_selected (DataFrame or array): Selected features of the validation dataset.\n",
        "    val_target (array): The actual target variable of the validation dataset.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary containing the precision, recall, and F1 score.\n",
        "    \"\"\"\n",
        "    y_pred = model.predict(X_val_selected)\n",
        "\n",
        "    precision = precision_score(val_target, y_pred, average='binary')\n",
        "    recall = recall_score(val_target, y_pred, average='binary')\n",
        "    f1 = f1_score(val_target, y_pred, average='binary')\n",
        "\n",
        "    return {\"Precision\": precision, \"Recall\": recall, \"F1 Score\": f1}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hyperparameter Optimization with Optuna for XGBoost Model**\n",
        "\n",
        "We focused on optimising the hyperparameters of an XGBoost classifier using the Optuna library. The goal is to find the best combination of hyperparameters that maximizes the F1 score of the model on the validation dataset. This is achieved through a systematic and efficient search across a specified range of hyperparameter values."
      ],
      "metadata": {
        "id": "WYRyLVvG9m19"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zE6OhtB0qyOM"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "import sklearn.metrics\n",
        "\n",
        "def optimize_hyperparameters(X_train_selected, y_resampled, X_val_selected, val_target, n_trials=100):\n",
        "    \"\"\"\n",
        "    Optimize hyperparameters for XGBoost using Optuna.\n",
        "\n",
        "    Parameters:\n",
        "    X_train_selected (DataFrame): Selected features of the training dataset.\n",
        "    y_resampled (array): The target array after resampling.\n",
        "    X_val_selected (DataFrame): Selected features of the validation dataset.\n",
        "    val_target (array): The actual target variable of the validation dataset.\n",
        "    n_trials (int, optional): The number of optimization trials. Default is 100.\n",
        "\n",
        "    Returns:\n",
        "    optuna.study.Study: The Optuna study object after completing the optimization.\n",
        "    \"\"\"\n",
        "\n",
        "    def objective(trial):\n",
        "        param = {\n",
        "            'objective': 'binary:logistic',\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "            'gamma': trial.suggest_float('gamma', 0.1, 5),\n",
        "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "            'lambda': trial.suggest_loguniform('lambda', 1e-4, 1.0),\n",
        "            'alpha': trial.suggest_loguniform('alpha', 1e-4, 1.0)\n",
        "        }\n",
        "\n",
        "        dtrain = xgb.DMatrix(X_train_selected, label=y_resampled)\n",
        "        dvalid = xgb.DMatrix(X_val_selected, label=val_target)\n",
        "\n",
        "        bst = xgb.train(param, dtrain)\n",
        "        preds = bst.predict(dvalid)\n",
        "        pred_labels = np.rint(preds)\n",
        "        f1 = sklearn.metrics.f1_score(val_target, pred_labels)\n",
        "        return f1\n",
        "\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(objective, n_trials=n_trials)\n",
        "\n",
        "    return study\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Studying Tuning Methods\n",
        "# study = optimize_hyperparameters(X_train_selected, y_resampled, X_val_selected, imputed_validation_set[target], n_trials=100)\n"
      ],
      "metadata": {
        "id": "W-TpK9zITTMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# study.best_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGGA4NAYHb0Y",
        "outputId": "02fc911e-412f-4714-d644-61f83c69ee2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'learning_rate': 0.22145013353887077,\n",
              " 'gamma': 4.3039462306959395,\n",
              " 'max_depth': 8,\n",
              " 'min_child_weight': 4,\n",
              " 'subsample': 0.6099674904240726,\n",
              " 'colsample_bytree': 0.6033130913572766,\n",
              " 'lambda': 0.0005851643553019946,\n",
              " 'alpha': 0.04115841889918572}"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### **Preprocessing Test Data (Used for unseen data)**\n",
        "These are used by the ```test_hidden_data``` function"
      ],
      "metadata": {
        "id": "vO2Oe0DQfhtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.experimental import enable_iterative_imputer  # Import to enable IterativeImputer\n",
        "from sklearn.impute import SimpleImputer, IterativeImputer\n",
        "import numpy as np\n",
        "\n",
        "def impute_validation_set(val, features):\n",
        "    \"\"\"\n",
        "    Imputes missing values in numeric and non-numeric features in the validation dataset.\n",
        "\n",
        "    Parameters:\n",
        "    val (DataFrame): The validation dataset.\n",
        "    features (list of str): The list of feature names.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: The imputed validation dataset.\n",
        "    \"\"\"\n",
        "    # Identify numeric and non-numeric features\n",
        "    numeric_features = val[features].select_dtypes(include=[np.number]).columns\n",
        "    non_numeric_features = val[features].select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "    # Initialize and fit Iterative Imputer for numeric features\n",
        "    imputer_numeric = IterativeImputer(max_iter=10, initial_strategy='median', random_state=0)\n",
        "    val_numeric_imputed = imputer_numeric.fit_transform(val[numeric_features])\n",
        "\n",
        "    # Initialize and fit Simple Imputer for non-numeric features\n",
        "    imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
        "    val_non_numeric_imputed = imputer_categorical.fit_transform(val[non_numeric_features])\n",
        "\n",
        "    # Update the DataFrame with imputed values\n",
        "    val[numeric_features] = val_numeric_imputed\n",
        "    val[non_numeric_features] = val_non_numeric_imputed\n",
        "\n",
        "    return val\n"
      ],
      "metadata": {
        "id": "uHKdqq3wfjrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectFromModel\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "def apply_feature_selection_validation(X_val_numeric, feature_selector):\n",
        "    \"\"\"\n",
        "    Applies the feature selection process to the validation dataset.\n",
        "\n",
        "    Parameters:\n",
        "    X_val_numeric (DataFrame): Numeric features of the validation dataset.\n",
        "    feature_selector (SelectFromModel): The feature selector fitted on the training data.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: The validation dataset with selected features.\n",
        "    \"\"\"\n",
        "    X_val_selected = feature_selector.transform(X_val_numeric)\n",
        "    return X_val_selected"
      ],
      "metadata": {
        "id": "2aakpEnTgW3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Training**\n",
        "\n",
        "The ```train_model``` function is designed to train a predictive model using XGBoost on a specified dataset. It performs several preprocessing steps including data cleaning, feature selection, and handling class imbalance, before training the model.\n",
        "\n",
        "## Usage:\n",
        "\n",
        "1. Prepare Your Dataset:\n",
        "As we are not saving our model, please run this function on the original ```catB_train.parquet``` dataset.\n",
        "\n",
        "2. Train the Model:\n",
        "Call the train_model function with the file path of your dataset. It returns a tuple containing the trained XGBoost model and the feature selector used during training.\n",
        "\n",
        "\n",
        "```\n",
        "model, feature_selector = train_model(\"path_to_your_dataset.parquet\")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "mD9xM770CRJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sklearn\n",
        "import xgboost as xgb\n",
        "\n",
        "def train_model(filePath):\n",
        "    \"\"\"\n",
        "    Trains a model on the data from the provided data file path.\n",
        "\n",
        "    Parameters:\n",
        "    filePath (str): The file path to the dataset.\n",
        "\n",
        "    Returns:\n",
        "    XGBClassifier: The trained XGBoost model.\n",
        "    \"\"\"\n",
        "    # Load the data from the provided file path\n",
        "    data = pd.read_parquet(filePath)\n",
        "    target = 'f_purchase_lh'\n",
        "\n",
        "    data_cleaned = clean_data(data, target)\n",
        "\n",
        "    # To further clean the data\n",
        "    data_cleaned = clean_data_v2(data_cleaned)\n",
        "\n",
        "    # Clean Target Column\n",
        "    data_cleaned = clean_target_column(data_cleaned, target)\n",
        "\n",
        "    # Create Age of User\n",
        "    calculate_age(data_cleaned, 'min_occ_date', 'cltdob_fix')\n",
        "\n",
        "     # Define the features\n",
        "    features = [col for col in data_cleaned.columns if col != target]\n",
        "\n",
        "    # Implement Data Imputation\n",
        "    imputed_set = impute_data(data_cleaned, features)\n",
        "\n",
        "    # Implement Under Over Sampling\n",
        "    X_resampled, y_resampled = under_over_sampling(imputed_set, features, target)\n",
        "\n",
        "    # Sample Numeric Data\n",
        "    X_sampled_numeric = extract_numeric_features(X_resampled, features)\n",
        "\n",
        "    # Feature Selection\n",
        "    X_train_selected, feature_selector, _ = select_features(X_sampled_numeric, y_resampled)\n",
        "\n",
        "    # Train the model\n",
        "    model = pretrain_model(X_train_selected, y_resampled)\n",
        "\n",
        "    return model, feature_selector"
      ],
      "metadata": {
        "id": "AgBEQwrmCRsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Function Invocation\n",
        "model, feature_selector = train_model(\"catB_train.parquet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzH4xlToCZFr",
        "outputId": "06f082fc-9d42-45a9-ccde-bd7483c21d03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-747012dde5f7>:34: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data[float_cols] = data[float_cols].astype('float32')\n",
            "<ipython-input-11-747012dde5f7>:37: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data.drop('clntnum', axis=1, inplace=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/impute/_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Testing Hidden Data**\n",
        "\n",
        "The ```testing_hidden_data``` function is used to apply a trained model to a test dataset. It handles preprocessing steps such as data cleaning and feature selection before making predictions.\n",
        "\n",
        "1. Prepare Your Test Dataset:\n",
        "The test dataset should be in a Parquet file format and contain the same structure and feature naming convention as the training dataset.\n",
        "\n",
        "2. Make Predictions:\n",
        "Use the function by providing the trained model, the feature selector, and the file path to your test dataset. The function returns the predictions and the true labels from the test set.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        " y_pred, y_test = testing_hidden_data(trained_model, feature_selector, \"path_to_test_data.parquet\")\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KbG76PQBCeP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def testing_hidden_data(model, feature_selector, testDatafilePath):\n",
        "    \"\"\"\n",
        "    Imputes missing values, applies feature selection, and makes predictions on a test dataset.\n",
        "\n",
        "    Parameters:\n",
        "    model (XGBClassifier): The trained XGBClassifier model.\n",
        "    X_test (DataFrame or array): Features of the test dataset.\n",
        "\n",
        "    Returns:\n",
        "    array: The predictions made by the model.\n",
        "    \"\"\"\n",
        "\n",
        "    target = 'f_purchase_lh'\n",
        "\n",
        "    if testDatafilePath.endswith('.parquet'):\n",
        "        X_test = pd.read_parquet(testDatafilePath)\n",
        "    elif testDatafilePath.endswith('.csv'):\n",
        "        X_test = pd.read_csv(testDatafilePath)\n",
        "\n",
        "    data_cleaned = clean_data(X_test, target)\n",
        "\n",
        "    # To further clean the data\n",
        "    data_cleaned = clean_data_v2(data_cleaned)\n",
        "\n",
        "    # Clean Target Column\n",
        "    data_cleaned = clean_target_column(data_cleaned, target)\n",
        "\n",
        "    # Create Age of User\n",
        "    calculate_age(data_cleaned, 'min_occ_date', 'cltdob_fix')\n",
        "\n",
        "    # Define the features\n",
        "    features = [col for col in data_cleaned.columns if col != target]\n",
        "\n",
        "    # Impute the test data\n",
        "    X_test_imputed = impute_validation_set(data_cleaned, features)\n",
        "\n",
        "    # Sample Numeric Data\n",
        "    X_test_imputed = extract_numeric_features(X_test_imputed, features)\n",
        "\n",
        "    # Apply feature selection\n",
        "    X_test_selected = apply_feature_selection_validation(X_test_imputed, feature_selector)\n",
        "\n",
        "    # test labels\n",
        "    y_test = data_cleaned[target]\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test_selected)\n",
        "\n",
        "    return y_pred, y_test  # Return the predictions and the test labels\n"
      ],
      "metadata": {
        "id": "pSIq7teWCdOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample Method Invocation\n",
        "\n",
        "#replace catB_train.parquet with the test file\n",
        "y_pred, y_test = testing_hidden_data(model, feature_selector, \"catB_train.parquet\")\n",
        "print(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxF2dE09CkDg",
        "outputId": "44a7d918-c92a-4a97-f7b5-f2a367984c16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-747012dde5f7>:34: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data[float_cols] = data[float_cols].astype('float32')\n",
            "<ipython-input-11-747012dde5f7>:37: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data.drop('clntnum', axis=1, inplace=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/impute/_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Metrics: Precision, Recall, and F1 Score\n",
        "\n",
        "**Precision** measures the accuracy of positive predictions. It is the ratio of true positive predictions to the total number of positive predictions (true positives + false positives). A higher precision score indicates that the model is more accurate in predicting positive cases.\n",
        "\n",
        "**Recall** (Sensitivity) measures the model's ability to correctly identify all positive cases. It is the ratio of true positive predictions to the actual number of positive cases (true positives + false negatives). Higher recall indicates that the model is better at catching all positive cases.\n",
        "\n",
        "**F1 Score** is the harmonic mean of precision and recall. It is a balance between precision and recall, providing a single metric that summarizes the model's accuracy. An F1 score reaches its best value at 1 (perfect precision and recall) and its worst at 0.\n",
        "\n",
        "These metrics are particularly useful in scenarios where classes are imbalanced or when the costs of false positives and false negatives are very different.\n",
        "\n"
      ],
      "metadata": {
        "id": "ICumhblPEBtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #Sample Method Invocation\n",
        "\n",
        "# from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# #Calculating precision, recall, and F1 score\n",
        "# precision = precision_score(y_test, y_pred, average='binary')  # Adjust 'average' parameter for multi-class\n",
        "# recall = recall_score(y_test, y_pred, average='binary')\n",
        "# f1 = f1_score(y_test, y_pred, average='binary')\n",
        "\n",
        "# # Printing the scores\n",
        "# print(\"Precision:\", precision)\n",
        "# print(\"Recall:\", recall)\n",
        "# print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GtJll1WEGGp",
        "outputId": "86c64b52-af09-484b-b584-b431c080fa4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.43037974683544306\n",
            "Recall: 0.384180790960452\n",
            "F1 Score: 0.4059701492537313\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}